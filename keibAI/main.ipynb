{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モジュールインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import dataclasses\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from types import MappingProxyType\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class UrlPaths:\n",
    "    DB_DOMAIN: str = 'https://db.netkeiba.com/'\n",
    "    # レース結果テーブル、レース情報テーブル、払い戻しテーブルが含まれるページ\n",
    "    RACE_URL: str = DB_DOMAIN + 'race/'\n",
    "    # 馬の過去成績テーブルが含まれるページ\n",
    "    HORSE_URL: str = DB_DOMAIN + 'horse/'\n",
    "    # 血統テーブルが含まれるページ\n",
    "    PED_URL: str = HORSE_URL + 'ped/'\n",
    "    \n",
    "    TOP_URL: str = 'https://race.netkeiba.com/top/'\n",
    "    # 開催日程ページ\n",
    "    CALENDAR_URL: str = TOP_URL + 'calendar.html'\n",
    "    # レース一覧ページ\n",
    "    RACE_LIST_URL: str = TOP_URL + 'race_list.html'\n",
    "    \n",
    "    # 出馬表ページ\n",
    "    SHUTUBA_TABLE: str = 'https://race.netkeiba.com/race/shutuba.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class LocalPaths:\n",
    "    # パス\n",
    "    ## プロジェクトルートの絶対パス\n",
    "    BASE_PATH: str = os.path.abspath('./')\n",
    "    ## dataディレクトリまでの絶対パス\n",
    "    DATA_PATH: str = os.path.join(os.path.abspath('./'),'data')\n",
    "    ### HTMLディレクトリのパス\n",
    "    HTML_PATH: str = os.path.join(DATA_PATH, 'html')\n",
    "    HTML_RACE_PATH: str = os.path.join(HTML_PATH, 'race')\n",
    "    HTML_HORSE_PATH: str = os.path.join(HTML_PATH, 'horse')\n",
    "    HTML_PED_PATH: str = os.path.join(HTML_PATH, 'ped')\n",
    "    \n",
    "    ### rawディレクトリのパス\n",
    "    RAW_PATH: str = os.path.join(DATA_PATH, 'raw')\n",
    "    RAW_RESULTS_PATH: str = os.path.join(RAW_PATH, 'results')\n",
    "    RAW_RACE_INFO_PATH: str = os.path.join(RAW_PATH, 'race_info')\n",
    "    RAW_RETURN_PATH: str = os.path.join(RAW_PATH, 'return_tables')\n",
    "    RAW_HORSE_RESULTS_PATH: str = os.path.join(RAW_PATH, 'horse_results')\n",
    "    RAW_PEDS_PATH: str = os.path.join(RAW_PATH, 'peds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 関数置き場\n",
    "## 各セルをそのまま実行してもらえれば大丈夫です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaisai_date(from_: str, to_: str):\n",
    "    date_range = pd.date_range(start=from_, end=to_, freq=\"M\")\n",
    "    kaisai_date_list = []\n",
    "    for year, month in tqdm(zip(date_range.year, date_range.month), total=len(date_range)):\n",
    "        query = [\n",
    "            'year=' + str(year),\n",
    "            'month=' + str(month),\n",
    "        ]\n",
    "        url = UrlPaths.CALENDAR_URL + '?' + '&'.join(query)\n",
    "        html = urlopen(url).read()\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        a_list = soup.find('table', class_='Calendar_Table').find_all('a')\n",
    "        for a in a_list:\n",
    "            kaisai_date_list.append(re.findall('(?<=kaisai_date=)\\d+', a['href'])[0])\n",
    "    return kaisai_date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_id_list(kaisai_date_list: list):\n",
    "    race_id_list = []\n",
    "    driver = webdriver.Chrome()\n",
    "    for kaisai_date in tqdm(kaisai_date_list):\n",
    "        try:\n",
    "            query = [\n",
    "                'kaisai_date=' + str(kaisai_date)\n",
    "            ]\n",
    "            url = UrlPaths.RACE_LIST_URL + '?' + '&'.join(query)\n",
    "            print('scraping: {}'.format(url))\n",
    "            driver.get(url)\n",
    "            try:\n",
    "                time.sleep(1) #取得の猶予として\n",
    "                a_list = driver.find_element(By.CLASS_NAME, 'RaceList_Box').find_elements(By.TAG_NAME, 'a')\n",
    "            except: #それでも取得できなかった場合\n",
    "                print('waiting more 10 seconds')\n",
    "                time.sleep(10)\n",
    "                a_list = driver.find_element(By.CLASS_NAME, 'RaceList_Box').find_elements(By.TAG_NAME, 'a')\n",
    "            for a in a_list:\n",
    "                race_id = re.findall('(?<=result.html\\?race_id=)\\d+', a.get_attribute('href'))\n",
    "                if len(race_id) > 0:\n",
    "                    race_id_list.append(race_id[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    driver.close()\n",
    "    return race_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_horse(horse_id_list: list, skip: bool = True):\n",
    "    \"\"\"\n",
    "    netkeiba.comのhorseページのhtmlをスクレイピングしてdata/html/horseに保存する関数。\n",
    "    \"\"\"\n",
    "    html_path_list = []\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        url = 'https://db.netkeiba.com/horse/' + horse_id #horse_idからurlを作る\n",
    "        html = urlopen(url).read() #スクレイピング実行\n",
    "        filename = 'data/html/horse/' + horse_id + '.bin'\n",
    "        html_path_list.append(filename)\n",
    "        if skip and os.path.isfile(filename): #skipがTrueで、かつbinファイルがすでに存在する場合は飛ばす\n",
    "            print('horse_id {} skipped'.format(horse_id))\n",
    "            continue\n",
    "        with open(filename, 'wb') as f: #保存するファイルパスを指定\n",
    "            f.write(html) #保存\n",
    "        time.sleep(1) #サーバーが落ちないようにするため必須\n",
    "    return html_path_list\n",
    "\n",
    "def get_html_race(race_id_list: list, skip: bool = True):\n",
    "    \"\"\"\n",
    "    netkeiba.comのraceページのhtmlをスクレイピングしてdata/html/raceに保存する関数。\n",
    "    \"\"\"\n",
    "    html_path_list = []\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        url = 'https://db.netkeiba.com/race/' + race_id #race_idからurlを作る\n",
    "        html = urlopen(url).read() #スクレイピング実行\n",
    "        filename = 'data/html/race/' + race_id + '.bin'\n",
    "        html_path_list.append(filename)\n",
    "        if skip and os.path.isfile(filename): #skipがTrueで、かつbinファイルがすでに存在する場合は飛ばす\n",
    "            print('race_id {} skipped'.format(race_id))\n",
    "            continue\n",
    "        with open(filename, 'wb') as f: #保存するファイルパスを指定\n",
    "            f.write(html) #保存\n",
    "        time.sleep(1) #サーバーが落ちないようにするため必須\n",
    "    return html_path_list\n",
    "\n",
    "def get_html_ped(horse_id_list: list, skip: bool = True):\n",
    "    \"\"\"\n",
    "    netkeiba.comのhorse/pedページのhtmlをスクレイピングしてdata/html/pedに保存する関数。\n",
    "    \"\"\"\n",
    "    html_path_list = []\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        url = 'https://db.netkeiba.com/horse/ped/' + horse_id #horse_idからurlを作る\n",
    "        html = urlopen(url).read() #スクレイピング実行\n",
    "        filename = 'data/html/ped/' + horse_id + '.bin'\n",
    "        html_path_list.append(filename)\n",
    "        if skip and os.path.isfile(filename): #skipがTrueで、かつbinファイルがすでに存在する場合は飛ばす\n",
    "            print('horse_id {} skipped'.format(horse_id))\n",
    "            continue\n",
    "        with open(filename, 'wb') as f: #保存するファイルパスを指定\n",
    "            f.write(html) #保存\n",
    "        time.sleep(1) #サーバーが落ちないようにするため必須\n",
    "    return html_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rawdata_results(html_path_list: list):\n",
    "    \"\"\"\n",
    "    raceページのhtmlを受け取って、レース結果テーブルに変換する関数。\n",
    "    \"\"\"\n",
    "    race_results = {}\n",
    "    for html_path in tqdm(html_path_list):\n",
    "        with open(html_path, 'rb') as f:\n",
    "            try:\n",
    "                html = f.read() #保存してあるbinファイルを読み込む\n",
    "                df = pd.read_html(html)[0] #メインとなるレース結果テーブルデータを取得\n",
    "                \n",
    "                soup = BeautifulSoup(html, \"html.parser\") #htmlをsoupオブジェクトに変換\n",
    "\n",
    "                #馬ID、騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "                )\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "                )\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df[\"horse_id\"] = horse_id_list\n",
    "                df[\"jockey_id\"] = jockey_id_list\n",
    "\n",
    "                #インデックスをrace_idにする\n",
    "                race_id = re.findall('(?<=race/)\\d+', html_path)[0]\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_results[race_id] = df\n",
    "            except Exception as e:\n",
    "                print('error at {}'.format(html_path))\n",
    "                print(e)\n",
    "    #pd.DataFrame型にして一つのデータにまとめる\n",
    "    race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "    return race_results_df\n",
    "\n",
    "def get_rawdata_info(html_path_list: list):\n",
    "    \"\"\"\n",
    "    raceページのhtmlを受け取って、レース情報テーブルに変換する関数。\n",
    "    \"\"\"\n",
    "    race_infos = {}\n",
    "    for html_path in tqdm(html_path_list):\n",
    "        with open(html_path, 'rb') as f:\n",
    "            try:\n",
    "                html = f.read() #保存してあるbinファイルを読み込む\n",
    "                \n",
    "                soup = BeautifulSoup(html, \"html.parser\") #htmlをsoupオブジェクトに変換\n",
    "\n",
    "                #天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                texts = (\n",
    "                    soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
    "                    + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
    "                )\n",
    "                info = re.findall(r'\\w+', texts)\n",
    "                df = pd.DataFrame()\n",
    "                for text in info:\n",
    "                    if text in [\"芝\", \"ダート\"]:\n",
    "                        df[\"race_type\"] = [text]\n",
    "                    if \"障\" in text:\n",
    "                        df[\"race_type\"] = [\"障害\"]\n",
    "                    if \"m\" in text:\n",
    "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[-1])] #20211212：[0]→[-1]に修正\n",
    "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                        df[\"ground_state\"] = [text]\n",
    "                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                        df[\"weather\"] = [text]\n",
    "                    if \"年\" in text:\n",
    "                        df[\"date\"] = [text]\n",
    "                \n",
    "                #インデックスをrace_idにする\n",
    "                race_id = re.findall('(?<=race/)\\d+', html_path)[0]\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_infos[race_id] = df\n",
    "            except Exception as e:\n",
    "                print('error at {}'.format(html_path))\n",
    "                print(e)\n",
    "    #pd.DataFrame型にして一つのデータにまとめる\n",
    "    race_infos_df = pd.concat([race_infos[key] for key in race_infos])\n",
    "\n",
    "    return race_infos_df\n",
    "\n",
    "def get_rawdata_return(html_path_list: list):\n",
    "    \"\"\"\n",
    "    raceページのhtmlを受け取って、払い戻しテーブルに変換する関数。\n",
    "    \"\"\"\n",
    "    horse_results = {}\n",
    "    for html_path in tqdm(html_path_list):\n",
    "        with open(html_path, 'rb') as f:\n",
    "            try: \n",
    "                html = f.read() #保存してあるbinファイルを読み込む\n",
    "                \n",
    "                html = html.replace(b'<br />', b'br')\n",
    "                dfs = pd.read_html(html)\n",
    "\n",
    "                #dfsの1番目に単勝〜馬連、2番目にワイド〜三連単がある\n",
    "                df = pd.concat([dfs[1], dfs[2]])\n",
    "                \n",
    "                race_id = re.findall('(?<=race/)\\d+', html_path)[0]\n",
    "                df.index = [race_id] * len(df)\n",
    "                horse_results[race_id] = df\n",
    "            except Exception as e:\n",
    "                print('error at {}'.format(html_path))\n",
    "                print(e)\n",
    "    #pd.DataFrame型にして一つのデータにまとめる\n",
    "    horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "    return horse_results_df\n",
    "\n",
    "def get_rawdata_horse_results(html_path_list: list):\n",
    "    \"\"\"\n",
    "    horseページのhtmlを受け取って、馬の過去成績のDataFrameに変換する関数。\n",
    "    \"\"\"\n",
    "    horse_results = {}\n",
    "    for html_path in tqdm(html_path_list):\n",
    "        with open(html_path, 'rb') as f:\n",
    "            html = f.read() #保存してあるbinファイルを読み込む\n",
    "            \n",
    "            df = pd.read_html(html)[3]\n",
    "            #受賞歴がある馬の場合、3番目に受賞歴テーブルが来るため、4番目のデータを取得する\n",
    "            if df.columns[0]=='受賞歴':\n",
    "                df = pd.read_html(html)[4]\n",
    "                \n",
    "            horse_id = re.findall('(?<=horse/)\\d+', html_path)[0]\n",
    "            \n",
    "            df.index = [horse_id] * len(df)\n",
    "            horse_results[horse_id] = df\n",
    "            \n",
    "    #pd.DataFrame型にして一つのデータにまとめる\n",
    "    horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "    return horse_results_df\n",
    "\n",
    "def get_rawdata_peds(html_path_list: list):\n",
    "    \"\"\"\n",
    "    horse/pedページのhtmlを受け取って、血統のDataFrameに変換する関数。\n",
    "    \"\"\"\n",
    "    peds = {}\n",
    "    for html_path in tqdm(html_path_list):\n",
    "        with open(html_path, 'rb') as f:\n",
    "            html = f.read() #保存してあるbinファイルを読み込む\n",
    "            \n",
    "            df = pd.read_html(html)[0]\n",
    "\n",
    "            #重複を削除して1列のSeries型データに直す\n",
    "            generations = {}\n",
    "            horse_id = re.findall('(?<=ped/)\\d+', html_path)[0]\n",
    "            for i in reversed(range(5)):\n",
    "                generations[i] = df[i]\n",
    "                df.drop([i], axis=1, inplace=True)\n",
    "                df = df.drop_duplicates()\n",
    "            ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "            peds[horse_id] = ped.reset_index(drop=True)\n",
    "    #pd.DataFrame型にして一つのデータにまとめる\n",
    "    peds_df = pd.concat([peds[key] for key in peds], axis=1).T.add_prefix('peds_')\n",
    "    return peds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.データ取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## レースID取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ※ここから自分で書き換える箇所あり\n",
    "#### （書き換え方）from_とto_の所で取得したい日時の範囲を指定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a277f6661074fddb11fe748e52b220d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#開催日取得\n",
    "kaisai_date_2023 = get_kaisai_date(from_=\"2022-01-01\", to_=\"2023-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （書き換え方）(kaisai_date_年数)として年数の部分を書き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e44f0fba4445ba8177b25ba9dfadfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181201\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181202\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181208\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181209\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181215\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181216\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181222\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181223\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20181228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 開催日からレースIDの取得とする\n",
    "race_id_list = get_race_id_list(kaisai_date_2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## raceデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd840adaf344563ba3923ab7e7ed65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['data/html/race/201806050101.bin',\n",
       " 'data/html/race/201806050102.bin',\n",
       " 'data/html/race/201806050103.bin',\n",
       " 'data/html/race/201806050104.bin',\n",
       " 'data/html/race/201806050105.bin']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://db.netkeiba.com/race/のhtml(binファイル)をスクレイピングして保存\n",
    "html_files_race = get_html_race(race_id_list)\n",
    "html_files_race[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48a0e4ead644a1098b78efc062a546d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc557502cc34495b14d9d93769747e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc8897f5e4b421e9550812d1940250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = get_rawdata_results(html_files_race) #レース結果テーブルの作成\n",
    "race_info = get_rawdata_info(html_files_race) #レース情報テーブルの作成\n",
    "return_table = get_rawdata_return(html_files_race) #払戻テーブルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （書き換え方）_年数の部分を書き換えると後で使いやすくなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#保存（分かりやすいように年数を書き換える）\n",
    "results.to_pickle('data/raw/results/results_2023.pickle')\n",
    "race_info.to_pickle('data/raw/race_info/race_info_2023.pickle')\n",
    "return_table.to_pickle('data/raw/return_tables/return_tables_2023.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## horseデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fcf75c336c440292bf9fc83ac4b029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "horse_id_list = results['horse_id'].unique()\n",
    "html_files_horse = get_html_horse(horse_id_list) #htmlをスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7950ba66f04744b0baca698b1e9729e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['/Users/yuta_watanabe/Downloads/scrape/data/html/horse/2016103911.bin',\n",
       " '/Users/yuta_watanabe/Downloads/scrape/data/html/horse/2016102397.bin',\n",
       " '/Users/yuta_watanabe/Downloads/scrape/data/html/horse/2016104661.bin',\n",
       " '/Users/yuta_watanabe/Downloads/scrape/data/html/horse/2016103526.bin',\n",
       " '/Users/yuta_watanabe/Downloads/scrape/data/html/horse/2016104091.bin']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_files_horse = []\n",
    "for horse_id in tqdm(horse_id_list):\n",
    "    file = glob.glob(os.path.join(LocalPaths.HTML_HORSE_PATH, horse_id+'*.bin'))[0]\n",
    "    html_files_horse.append(file)\n",
    "html_files_horse[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （書き換え方）同様に_年数の部分を書き換えると後で扱いやすい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a947f3d3b3cf400b84de6b7537f02853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#分かりやすいように年数を書き換える\n",
    "horse_results_2023 = get_rawdata_horse_results(html_files_horse) #馬の過去成績テーブルの作成\n",
    "horse_results_2023.to_pickle('data/raw/horse_results/horse_results_2023.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pedデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99f96242da445c8e34fcb7a9838d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_files_peds = get_html_ped(horse_id_list) #htmlをスクレイピング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （書き換え方）同様に_年数の部分を書き換えると後で扱いやすい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4efd3872fe34e62a545ad2a88ead36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#分かりやすいように年数を書き換える\n",
    "peds_2023 = get_rawdata_peds(html_files_peds) #血統テーブルの作成\n",
    "peds_2023.to_pickle('data/raw/peds/peds_2023.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数置き場"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractDataProcessor(metaclass=ABCMeta):\n",
    "    def __init__(self, path_list):\n",
    "        self.__raw_data = self._read_pickle(path_list)\n",
    "        self.__preprocessed_data = self._preprocess()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _preprocess(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def raw_data(self):\n",
    "        return self.__raw_data.copy()\n",
    "\n",
    "    @property\n",
    "    def preprocessed_data(self):\n",
    "        return self.__preprocessed_data.copy()\n",
    "\n",
    "    def _delete_duplicate(self, old, new):\n",
    "        filtered_old = old[~old.index.isin(new.index)]\n",
    "        return pd.concat([filtered_old, new])\n",
    "\n",
    "    def _read_pickle(self, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = self._delete_duplicate(df, pd.read_pickle(path))\n",
    "        return df\n",
    "\n",
    "class ResultsProcessor(AbstractDataProcessor):\n",
    "    def __init__(self, path_list):\n",
    "        super().__init__(path_list)\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        df = self.raw_data.copy()\n",
    "        \n",
    "        df = self._preprocess_rank(df)\n",
    "        \n",
    "        # 性齢を性と年齢に分ける\n",
    "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0]\n",
    "        df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1]\n",
    "        \n",
    "        #errors='coerce'で、\"計不\"など変換できない時に欠損値にする\n",
    "        df['体重'] = pd.to_numeric(df['体重'], errors='coerce')\n",
    "        df['体重変化'] = pd.to_numeric(df['体重変化'], errors='coerce')\n",
    "\n",
    "        # 各列を数値型に変換\n",
    "        df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
    "        df[\"斤量\"] = df[\"斤量\"].astype(float)\n",
    "        df[\"枠番\"] = df[\"枠番\"].astype(int)\n",
    "        df[\"馬番\"] = df[\"馬番\"].astype(int)\n",
    "        \n",
    "        #6/6出走数追加\n",
    "        df['n_horses'] = df.index.map(df.index.value_counts())\n",
    "        \n",
    "        df = self._select_columns(df)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "        \n",
    "    def _preprocess_rank(self, raw):\n",
    "        df = raw.copy()\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        df['rank'] = df['着順'].map(lambda x:1 if x<4 else 0)\n",
    "        return df\n",
    "    \n",
    "    def _select_columns(self, raw):\n",
    "        df = raw.copy()[[\n",
    "            '枠番','馬番','斤量','単勝','horse_id','jockey_id','性', '年齢','体重','体重変化','n_horses', 'rank'\n",
    "            ]]\n",
    "        return df\n",
    "\n",
    "class RaceInfoProcessor(AbstractDataProcessor):\n",
    "    def __init__(self, path_list):\n",
    "        super().__init__(path_list)\n",
    "        \n",
    "    def _preprocess(self):\n",
    "        df = self.raw_data\n",
    "        # 距離は10の位を切り捨てる\n",
    "        df[\"course_len\"] = df[\"course_len\"].astype(float) // 100\n",
    "        # 日付型に変更\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y年%m月%d日\")\n",
    "        # 開催場所\n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "class ReturnProcessor(AbstractDataProcessor):\n",
    "    def __init__(self, path_list):\n",
    "        super().__init__(path_list)\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        return_dict = {}\n",
    "        return_dict['tansho'] = self.__tansho()\n",
    "        return_dict['fukusho'] = self.__fukusho()\n",
    "        return_dict['umaren'] = self.__umaren()\n",
    "        return_dict['umatan'] = self.__umatan()\n",
    "        return_dict['wide'] = self.__wide()\n",
    "        return_dict['sanrentan'] = self.__sanrentan()\n",
    "        return_dict['sanrenpuku'] = self.__sanrenpuku()        \n",
    "        return return_dict\n",
    "    \n",
    "    def __tansho(self):\n",
    "        tansho = self.raw_data[self.raw_data[0]=='単勝'][[1,2]]\n",
    "        tansho.columns = ['win', 'return']\n",
    "        \n",
    "        for column in tansho.columns:\n",
    "            tansho[column] = pd.to_numeric(tansho[column], errors='coerce')\n",
    "            \n",
    "        return tansho\n",
    "    \n",
    "    def __fukusho(self):\n",
    "        fukusho = self.raw_data[self.raw_data[0]=='複勝'][[1,2]]\n",
    "        wins = fukusho[1].str.split('br', expand=True)[[0,1,2]]\n",
    "        \n",
    "        wins.columns = ['win_0', 'win_1', 'win_2']\n",
    "        returns = fukusho[2].str.split('br', expand=True)[[0,1,2]]\n",
    "        returns.columns = ['return_0', 'return_1', 'return_2']\n",
    "        \n",
    "        df = pd.concat([wins, returns], axis=1)\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',', '')\n",
    "        return df.fillna(0).astype(int)\n",
    "    \n",
    "    \n",
    "    def __umaren(self):\n",
    "        umaren = self.raw_data[self.raw_data[0]=='馬連'][[1,2]]\n",
    "        wins = umaren[1].str.split('-', expand=True)[[0,1]].add_prefix('win_')\n",
    "        return_ = umaren[2].rename('return')  \n",
    "        df = pd.concat([wins, return_], axis=1)        \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    \n",
    "    def __umatan(self):\n",
    "        umatan = self.raw_data[self.raw_data[0]=='馬単'][[1,2]]\n",
    "        wins = umatan[1].str.split('→', expand=True)[[0,1]].add_prefix('win_')\n",
    "        return_ = umatan[2].rename('return')  \n",
    "        df = pd.concat([wins, return_], axis=1)        \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    \n",
    "    def __wide(self):\n",
    "        wide = self.raw_data[self.raw_data[0]=='ワイド'][[1,2]]\n",
    "        wins = wide[1].str.split('br', expand=True)[[0,1,2]]\n",
    "        wins = wins.stack().str.split('-', expand=True).add_prefix('win_')\n",
    "        return_ = wide[2].str.split('br', expand=True)[[0,1,2]]\n",
    "        return_ = return_.stack().rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1)\n",
    "        return df.apply(lambda x: pd.to_numeric(x.str.replace(',',''), errors='coerce'))\n",
    "    \n",
    "    \n",
    "    def __sanrentan(self):\n",
    "        rentan = self.raw_data[self.raw_data[0]=='三連単'][[1,2]]\n",
    "        wins = rentan[1].str.split('→', expand=True)[[0,1,2]].add_prefix('win_')\n",
    "        return_ = rentan[2].rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1) \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    \n",
    "    def __sanrenpuku(self):\n",
    "        renpuku = self.raw_data[self.raw_data[0]=='三連複'][[1,2]]\n",
    "        wins = renpuku[1].str.split('-', expand=True)[[0,1,2]].add_prefix('win_')\n",
    "        return_ = renpuku[2].rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1) \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "class HorseResultsProcessor(AbstractDataProcessor):\n",
    "    def __init__(self, path_list):\n",
    "        super().__init__(path_list)\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        df = self.raw_data\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "        \n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "        \n",
    "        #1着の着差を0にする\n",
    "        df['着差'] = df['着差'].map(lambda x: 0 if x<0 else x)\n",
    "        \n",
    "        #レース展開データ\n",
    "        #n=1: 最初のコーナー位置, n=4: 最終コーナー位置\n",
    "        def corner(x, n):\n",
    "            if type(x) != str:\n",
    "                return x\n",
    "            elif n==4:\n",
    "                return int(re.findall(r'\\d+', x)[-1])\n",
    "            elif n==1:\n",
    "                return int(re.findall(r'\\d+', x)[0])\n",
    "        df['first_corner'] = df['通過'].map(lambda x: corner(x, 1))\n",
    "        df['final_corner'] = df['通過'].map(lambda x: corner(x, 4))\n",
    "        \n",
    "        df['final_to_rank'] = df['final_corner'] - df['着順']\n",
    "        df['first_to_rank'] = df['first_corner'] - df['着順']\n",
    "        df['first_to_final'] = df['first_corner'] - df['final_corner']\n",
    "        \n",
    "        #開催場所\n",
    "        df['開催'] = df['開催'].str.extract(r'(\\D+)')[0].map(master.PLACE_DICT).fillna('11')\n",
    "        #race_type\n",
    "        df['race_type'] = df['距離'].str.extract(r'(\\D+)')[0].map(master.RACE_TYPE_DICT)\n",
    "        #距離は10の位を切り捨てる\n",
    "        df['course_len'] = df['距離'].str.extract(r'(\\d+)').astype(int) // 100\n",
    "        df.drop(['距離'], axis=1, inplace=True)\n",
    "        #インデックス名を与える\n",
    "        df.index.name = 'horse_id'\n",
    "        \n",
    "        return df\n",
    "\n",
    "class PedsProcessor(AbstractDataProcessor):\n",
    "    def __init__(self, path_list):\n",
    "        super().__init__(path_list)\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        df = self.raw_data\n",
    "        for column in df.columns:\n",
    "            df[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))\n",
    "        return df.astype('category')\n",
    " \n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class Master:\n",
    "    PLACE_DICT: dict = MappingProxyType({\n",
    "        '札幌':'01',\n",
    "        '函館':'02',\n",
    "        '福島':'03',\n",
    "        '新潟':'04',\n",
    "        '東京':'05',\n",
    "        '中山':'06',\n",
    "        '中京':'07',\n",
    "        '京都':'08',\n",
    "        '阪神':'09',\n",
    "        '小倉':'10',\n",
    "        })\n",
    "\n",
    "    RACE_TYPE_DICT: dict = MappingProxyType({\n",
    "        '芝': '芝',\n",
    "        'ダ': 'ダート',\n",
    "        '障': '障害',\n",
    "        })\n",
    "    \n",
    "    WEATHER_LIST: tuple = ('晴', '曇', '小雨', '雨', '小雪', '雪')\n",
    "    \n",
    "    GROUND_STATE_LIST: tuple = ('良', '稍重', '重', '不良')\n",
    "    \n",
    "    SEX_LIST: tuple = ('牡', '牝', 'セ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    \"\"\"\n",
    "    使うテーブルを全てマージした後の処理をするクラス。\n",
    "    新しい特徴量を作りたいときは、メソッド単位で追加していく。\n",
    "    各メソッドは依存関係を持たないよう注意。\n",
    "    \"\"\"\n",
    "    def __init__(self, data_merger: DataMerger):\n",
    "        self.__data = data_merger.merged_data.copy()\n",
    "        \n",
    "    @property\n",
    "    def featured_data(self):\n",
    "        return self.__data\n",
    "    \n",
    "    def add_interval(self):\n",
    "        \"\"\"\n",
    "        前走からの経過日数\n",
    "        \"\"\"\n",
    "        self.__data['interval'] = (self.__data['date'] - self.__data['latest']).dt.days\n",
    "        self.__data.drop('latest', axis=1, inplace=True)\n",
    "        return self\n",
    "    \n",
    "    def dumminize_weather(self):\n",
    "        \"\"\"\n",
    "        weatherカラムをダミー変数化する\n",
    "        \"\"\"\n",
    "        self.__data[Cols.WEATHER] = pd.Categorical(self.__data[Cols.WEATHER], Master.WEATHER_LIST)\n",
    "        self.__data = pd.get_dummies(self.__data, columns=[Cols.WEATHER])\n",
    "        return self\n",
    "    \n",
    "    def dumminize_race_type(self):\n",
    "        \"\"\"\n",
    "        race_typeカラムをダミー変数化する\n",
    "        \"\"\"\n",
    "        self.__data[Cols.RACE_TYPE] = pd.Categorical(\n",
    "            self.__data[Cols.RACE_TYPE], list(Master.RACE_TYPE_DICT.values())\n",
    "            )\n",
    "        self.__data = pd.get_dummies(self.__data, columns=[Cols.RACE_TYPE])\n",
    "        return self\n",
    "    \n",
    "    def dumminize_ground_state(self):\n",
    "        \"\"\"\n",
    "        ground_stateカラムをダミー変数化する\n",
    "        \"\"\"\n",
    "        self.__data[Cols.GROUND_STATE] = pd.Categorical(\n",
    "            self.__data[Cols.GROUND_STATE], Master.GROUND_STATE_LIST\n",
    "            )\n",
    "        self.__data = pd.get_dummies(self.__data, columns=[Cols.GROUND_STATE])\n",
    "        return self\n",
    "    \n",
    "    def dumminize_sex(self):\n",
    "        \"\"\"\n",
    "        sexカラムをダミー変数化する\n",
    "        \"\"\"\n",
    "        self.__data[Cols.SEX] = pd.Categorical(self.__data[Cols.SEX], Master.SEX_LIST)\n",
    "        self.__data = pd.get_dummies(self.__data, columns=[Cols.SEX])\n",
    "        return self\n",
    "    \n",
    "    def encode_horse_id(self):\n",
    "        \"\"\"\n",
    "        horse_idをラベルエンコーディングして、Categorical型に変換する。\n",
    "        \"\"\"\n",
    "        csv_path = 'data/master/horse_id.csv'\n",
    "        horse_master = pd.read_csv(csv_path, dtype=object)\n",
    "        new_horses = self.__data[[Cols.HORSE_ID]][\n",
    "            ~self.__data[Cols.HORSE_ID].isin(horse_master['horse_id'])\n",
    "            ].drop_duplicates(subset=['horse_id'])\n",
    "        new_horses['encoded_id'] = [i+len(horse_master) for i in range(len(new_horses))]\n",
    "        new_horse_master = pd.concat([horse_master, new_horses]).set_index('horse_id')['encoded_id']\n",
    "        new_horse_master.to_csv(csv_path)\n",
    "        self.__data[Cols.HORSE_ID] = pd.Categorical(self.__data[Cols.HORSE_ID].map(new_horse_master))\n",
    "        return self\n",
    "    \n",
    "    def encode_jockey_id(self):\n",
    "        \"\"\"\n",
    "        jockey_idをラベルエンコーディングして、Categorical型に変換する。\n",
    "        \"\"\"\n",
    "        csv_path = 'data/master/jockey_id.csv'\n",
    "        jockey_master = pd.read_csv(csv_path, dtype=object)\n",
    "        new_jockeys = self.__data[[Cols.JOCKEY_ID]][\n",
    "            ~self.__data[Cols.JOCKEY_ID].isin(jockey_master['jockey_id'])\n",
    "            ].drop_duplicates(subset=['jockey_id'])\n",
    "        new_jockeys['encoded_id'] = [i+len(jockey_master) for i in range(len(new_jockeys))]\n",
    "        new_jockey_master = pd.concat([jockey_master, new_jockeys]).set_index('jockey_id')['encoded_id']\n",
    "        new_jockey_master.to_csv(csv_path)\n",
    "        self.__data[Cols.JOCKEY_ID] = pd.Categorical(self.__data[Cols.JOCKEY_ID].map(new_jockey_master))\n",
    "        return self\n",
    "    \n",
    "    def dumminize_kaisai(self):\n",
    "        self.__data[Cols.KAISAI] = pd.Categorical(\n",
    "            self.__data[Cols.KAISAI], list(Master.PLACE_DICT.values())\n",
    "            )\n",
    "        self.__data = pd.get_dummies(self.__data, columns=[Cols.KAISAI])\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d01822c593d7f4bad3785e75425d9cd0d468257df53a4832c45dec1669d37d75"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
